# Autoencoders for Constructing Language

## Datasets

Datasets are saved as tensor datasets where each sample is represented by a single dimensional tensor of tokens. Samples should be padded to the same length by using padding tokens (0). Datasets should be saved as PyTorch `.pt` files.

`preprocess.py` is an example for processing a text file into datasets ready to be used for training and validation.

**TODO** Try to create a more concrete inferface for encoding sentences into token sequences and combining arbitrarily many sequences into datasets. It would be nice to have some sort of Tokenizer class that we can save for each dataset we generate so that it's easy to tokenize new outputs during inference into input of the same format as the training dataset.

My thoughts of how this could work:
- Represent tokenizers as `.yaml` config files by generalizing features that many tokenizers might have
- Python file to encode an arbitrary text file as a dataset and save as `.pt`: `tokenize.py tokenizer.yaml text.txt path/to/output`
- Python file to convert another arbitrary text file using a trained model and save as `.pt`: `convert.py tokenizer.yaml model.pt text.txt path/to/output`
- Python file to interactively translate a sentence at a time (input through the command line?): `translate.py tokenizer.yaml model.pt`
  - After running we could enter a sentence in english and get as output the corresponding translation in numerical tokens
```
>>> I want to eat an apple.
Translation: 45 2 90 67 4 5 78 90 1
>>> I want an apple to eat me.
Translation: 45 2 90 78 4 5 67 23 90 1
```
## Training

A training job can be started by creating a `.yaml` config file and running `train.py spec.yaml`.

An example `.yaml` file is shown below:
```
lang:
  input:
    vocab_size: 2048
    seq_len: 32
  output:
    vocab_size: 64
    seq_len: 32
dataset:
  train: ./starwars/datasets/ep4/train.pt
  val: ./starwars/datasets/ep4/val.pt
model:
  d_model: 256
  nhead: 4
  num_encoder_layers: 3
  num_decoder_layers: 3
  dim_feedforward: 2048
  temperature: 0.1
train:
  path: ./starwars/models/ep4/test.pt
  lr: 0.0001
  weight_decay: 0.0001
  dropout: 0.1
  discrete: False
  batch_size: 256
  epochs: 250
  clip_grad_norm: 1.0
```

Model specification parameters include:
- *lang.input* denotes the language of the training data
- *lang.output* denotes the constructed language generated by the model
- *vocab_size* denotes the range of tokens pertaining to the language
- *seq_len* denotes the maximum length of a sentence unit in the language
- *temperature* specifies how "random" we want samples to be (higher means more random)
- *train.path* is the path where the finished model will be saved
- *discrete* specifies whether or not samples from the constructed language are discrete tokens or Gumbel-softmax combinations 

**TODO** Add better training tools:
- Saving the top 1 model by validation loss
- Generating plots of loss vs batch
- Creating tools for hyperparameter tuning

## Evaluation

**TODO**
